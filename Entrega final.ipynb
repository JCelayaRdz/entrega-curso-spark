{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e4b1c8",
   "metadata": {},
   "source": [
    "# Creamos la sesión de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327ce2f",
   "metadata": {},
   "source": [
    "Como este notebook se ejecuta en un entorno de Anaconda que tiene todas las dependencias necesarias, podremos importar directamente la clase `SparkSession`, que, a continuación, instanciaremos en una variable llamada `spark`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c303ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-RPGQ1TT:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CursoSpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21d909034c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"CursoSpark\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40045637",
   "metadata": {},
   "source": [
    "# Cargamos el fichero CSV en un DataFrame de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0baebe",
   "metadata": {},
   "source": [
    "A continuación cargamos el fichero CSV usando la variable `spark`, instanciada en el apartado anterior. A la hora de leer el fichero CSV usaremos dos opciones `header` = `True` y `inferSchema` = `True`, la primera para que las columnas del DataFrame tengan nombre y la segunda para que no todos los datos del CSV sean interpretados como string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d8a299",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dateRep: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- cases: integer (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      " |-- countriesAndTerritories: string (nullable = true)\n",
      " |-- geoId: string (nullable = true)\n",
      " |-- countryterritoryCode: string (nullable = true)\n",
      " |-- popData2018: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df = spark.read \\\n",
    "                .option('header', True) \\\n",
    "                .option('inferSchema', True) \\\n",
    "                .csv('./covid.csv')\n",
    "\n",
    "covid_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6813324",
   "metadata": {},
   "source": [
    "Una vez cargado el fichero CSV, imprimimos las 3 primeras filas del DataFrame usando el metodo `show`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b979544e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+\n",
      "|   dateRep|day|month|year|cases|deaths|countriesAndTerritories|geoId|countryterritoryCode|popData2018|\n",
      "+----------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+\n",
      "|29/03/2020| 29|    3|2020|   15|     1|            Afghanistan|   AF|                 AFG|   37172386|\n",
      "|28/03/2020| 28|    3|2020|   16|     1|            Afghanistan|   AF|                 AFG|   37172386|\n",
      "|27/03/2020| 27|    3|2020|    0|     0|            Afghanistan|   AF|                 AFG|   37172386|\n",
      "+----------+---+-----+----+-----+------+-----------------------+-----+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68184a3e",
   "metadata": {},
   "source": [
    "# Consultas sobre el DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf27b1",
   "metadata": {},
   "source": [
    "En este apartado se realizarán consultas analíticas sobre el DataFrame usando la API de PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa350f8",
   "metadata": {},
   "source": [
    "¿Cuántos casos de COVID había en España el 15 de marzo de 2020 (la fecha en la que comenzó el confinamiento)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5628778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "| Pais|     Fecha|Casos|\n",
      "+-----+----------+-----+\n",
      "|Spain|15/03/2020| 1522|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "covid_df.where(col('dateRep') == '15/03/2020') \\\n",
    "        .where(col('countriesAndTerritories') == 'Spain') \\\n",
    "        .select(\n",
    "            col('countriesAndTerritories').alias('Pais'),\n",
    "            col('dateRep').alias('Fecha'),\n",
    "            col('cases').alias('Casos')) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de7f31",
   "metadata": {},
   "source": [
    "¿Cuántas muertes se registraron en España aquellos días en los que el número de casos es el mínimo(en España)? ¿En qué fechas ocurrió? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5e9bd",
   "metadata": {},
   "source": [
    "La consulta en SQL correspondiente sería:\n",
    "```sql\n",
    "    SELECT Pais, Fecha, Muertes\n",
    "    FROM Covid\n",
    "    WHERE Casos = (SELECT MIN(Casos) FROM Covid WHERE Pais = 'Spain')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8718367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "| Pais|     Fecha|Muertes|\n",
      "+-----+----------+-------+\n",
      "|Spain|24/02/2020|      0|\n",
      "|Spain|23/02/2020|      0|\n",
      "|Spain|22/02/2020|      0|\n",
      "|Spain|21/02/2020|      0|\n",
      "|Spain|20/02/2020|      0|\n",
      "|Spain|19/02/2020|      0|\n",
      "|Spain|18/02/2020|      0|\n",
      "|Spain|17/02/2020|      0|\n",
      "|Spain|16/02/2020|      0|\n",
      "|Spain|15/02/2020|      0|\n",
      "|Spain|14/02/2020|      0|\n",
      "|Spain|13/02/2020|      0|\n",
      "|Spain|12/02/2020|      0|\n",
      "|Spain|11/02/2020|      0|\n",
      "|Spain|09/02/2020|      0|\n",
      "|Spain|08/02/2020|      0|\n",
      "|Spain|07/02/2020|      0|\n",
      "|Spain|06/02/2020|      0|\n",
      "|Spain|05/02/2020|      0|\n",
      "|Spain|04/02/2020|      0|\n",
      "+-----+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "\n",
    "subquery = covid_df \\\n",
    "    .where(col('countriesAndTerritories') == 'Spain') \\\n",
    "    .select(min('cases').alias('min_cases'))\n",
    "\n",
    "covid_df.join(subquery, col('cases') == col('min_cases')) \\\n",
    "        .where(col('countriesAndTerritories') == 'Spain') \\\n",
    "        .select(\n",
    "            col('countriesAndTerritories').alias('Pais'),\n",
    "            col('dateRep').alias('Fecha'),\n",
    "            col('deaths').alias('Muertes')) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf193bc3",
   "metadata": {},
   "source": [
    "Obtener el máximo número de casos de COVID que tuvo cada país, ordenados de manera descendente por el número de casos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a530576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                Pais|maxCasos|\n",
      "+--------------------+--------+\n",
      "|United_States_of_...|   19979|\n",
      "|               China|   15141|\n",
      "|               Spain|    8578|\n",
      "|               Italy|    6557|\n",
      "|             Germany|    6294|\n",
      "|              France|    4611|\n",
      "|                Iran|    3076|\n",
      "|      United_Kingdom|    2885|\n",
      "|              Turkey|    2069|\n",
      "|             Belgium|    1850|\n",
      "|              Canada|    1426|\n",
      "|         Switzerland|    1390|\n",
      "|         Netherlands|    1172|\n",
      "|             Austria|    1141|\n",
      "|         South_Korea|     909|\n",
      "|            Portugal|     902|\n",
      "|           Australia|     611|\n",
      "|              Israel|     584|\n",
      "|              Brazil|     502|\n",
      "|              Norway|     425|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid_df.groupBy('countriesAndTerritories') \\\n",
    "        .max() \\\n",
    "        .select(\n",
    "            col('countriesAndTerritories').alias('Pais'),\n",
    "            col('max(cases)').alias('maxCasos')) \\\n",
    "        .orderBy('maxCasos', ascending=False) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5fa002",
   "metadata": {},
   "source": [
    "# Consultas usando sentencias SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c4aa2",
   "metadata": {},
   "source": [
    "Primero debemos de crear una vista temporal a la que llamaremos `Covid`. Las consultas SQL se realizarán sobre esta vista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679055b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.createOrReplaceTempView('Covid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e804a1",
   "metadata": {},
   "source": [
    "¿En qué fecha España registro el mayor número de casos? ¿Cuántos casos tuvo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f047d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "| Pais|     Fecha|Casos|\n",
      "+-----+----------+-----+\n",
      "|Spain|27/03/2020| 8578|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT countriesAndTerritories AS Pais, dateRep AS Fecha, cases AS Casos\n",
    "            FROM Covid\n",
    "            WHERE cases = (SELECT MAX(cases) FROM Covid WHERE countriesAndTerritories = 'Spain')\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523cfc1",
   "metadata": {},
   "source": [
    "¿Hubo algún país que nunca registrara ningún caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da7b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Pais|\n",
      "+----+\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT countriesAndTerritories AS Pais\n",
    "            FROM Covid\n",
    "            GROUP BY countriesAndTerritories\n",
    "            HAVING SUM(cases) = 0\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697af37",
   "metadata": {},
   "source": [
    "¿Qué país registro el mayor número de muertes? ¿Cuántas fueron? ¿En qué fecha fue? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2e72922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "| Pais|     Fecha|Muertes|\n",
      "+-----+----------+-------+\n",
      "|Italy|28/03/2020|    971|\n",
      "+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT countriesAndTerritories AS Pais, dateRep AS Fecha, deaths AS Muertes\n",
    "            FROM Covid\n",
    "            WHERE deaths = (SELECT MAX(deaths) FROM Covid)\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0cb9a3",
   "metadata": {},
   "source": [
    "# Convertimos el DataFrame en un archivo Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff3313",
   "metadata": {},
   "source": [
    "**IMPORTANTE:** En el curso se utilizaba una máquina virtual que contenía todas las librerías necesarias. Este repositorio ha sido adaptado para que el notebook se pueda ejecutar en un entorno de Anaconda, es por ello que no se podrá ejecutar código que requiera de las funciones de Hadoop, como por ejemplo convertir un DataFrame a un archivo parquet, aun así he dejado el código correspondiente, pero comentado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b18eb0",
   "metadata": {},
   "source": [
    "En esta sección convertiremos el DataFrame en un archivo Parquet que se guardará en la ruta parquet/ de la máquina virtual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d929532d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncovid_df.write         .format('parquet')         .mode('overwrite')         .save('file:////home/training/parquet/')\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "covid_df.write \\\n",
    "        .format('parquet') \\\n",
    "        .mode('overwrite') \\\n",
    "        .save('file:////home/training/parquet/')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef42cd",
   "metadata": {},
   "source": [
    "# Cerramos la sesión de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b31dfbe",
   "metadata": {},
   "source": [
    "Finalmente, cerramos la sesión de Spark con el método `stop` de la variable `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae9434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
